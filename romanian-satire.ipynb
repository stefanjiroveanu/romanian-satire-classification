{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a65d5d2-35d7-4a51-8dab-1741d222aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c3ebee6-0214-49ae-af12-22368e7ea6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy random seed set with value: 42\n",
      "TensorFlow random seed was not set: No module named 'tensorflow'\n",
      "PyTorch random seed set with value: 42\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def reset_numpy_seed(seed_value=42):\n",
    "  try:\n",
    "    # Set NumPy random seed\n",
    "    import numpy as np\n",
    "    np.random.seed(seed_value)\n",
    "    print(f'NumPy random seed set with value: {seed_value}')\n",
    "  except Exception as e:\n",
    "    print(f'NumPy random seed was not set: {e}')\n",
    "  return\n",
    "\n",
    "\n",
    "def reset_tensorflow_seed(seed_value=42):\n",
    "  try:\n",
    "    # Set TensorFlow random seed\n",
    "    import tensorflow as tf\n",
    "    success = False\n",
    "    # Here we have 2 different ways to set the seed\n",
    "    # depending on the version of TensorFlow\n",
    "    try:\n",
    "      tf.random.set_seed(seed_value)\n",
    "      success = True\n",
    "    except Exception as e:\n",
    "      pass\n",
    "    try:\n",
    "      tf.set_random_seed(seed_value)\n",
    "      success = True\n",
    "    except Exception as e:\n",
    "      pass\n",
    "    if success:\n",
    "      print(f'TensorFlow random seed set with value: {seed_value}')\n",
    "    else:\n",
    "      print(f'TensorFlow random seed was not set')\n",
    "  except Exception as e:\n",
    "    print(f'TensorFlow random seed was not set: {e}')\n",
    "  return\n",
    "\n",
    "\n",
    "def reset_torch_seed(seed_value=42):\n",
    "  try:\n",
    "    # Set PyTorch random seed\n",
    "    import torch\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "      torch.cuda.manual_seed(seed_value)\n",
    "      torch.cuda.manual_seed_all(seed_value)  # if you are using multiple GPUs\n",
    "    print(f'PyTorch random seed set with value: {seed_value}')\n",
    "  except Exception as e:\n",
    "    print(f'PyTorch random seed was not set: {e}')\n",
    "  return\n",
    "\n",
    "\n",
    "def set_random_seeds(seed_value=42):\n",
    "  # Set Python random seed\n",
    "  random.seed(seed_value)\n",
    "  reset_numpy_seed(seed_value)\n",
    "  reset_tensorflow_seed(seed_value)\n",
    "  reset_torch_seed(seed_value)\n",
    "  return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # Set the desired seed value\n",
    "  seed = 42\n",
    "\n",
    "  # Set random seeds\n",
    "  set_random_seeds(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "929fd908-7ce8-45e6-bfdc-63a59c7882d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44df9a0b-f0f3-4713-a6a9-f380921e9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, logging\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4094b23f-43bd-44a7-866a-f8c221c6164c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"racai/distilbert-base-romanian-cased\", cache_dir=\".cache/huggingface\", use_fast=True)\n",
    "special_tokens_dict = {'additional_special_tokens': ['[MISSING_TITLE]', '[MISSING_CONTENT]']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "693fbd6a-e6e3-4b0a-9245-c063090dd6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import AutoModel\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "class RomanianBertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, num_labels=2, class_weights=None):\n",
    "        super(RomanianBertForSequenceClassification, self).__init__()\n",
    "        self.distilbert = AutoModel.from_pretrained(\n",
    "            \"racai/distilbert-base-romanian-cased\",\n",
    "            cache_dir=\".cache/huggingface\"\n",
    "        )\n",
    "        vocab_size = self.distilbert.config.vocab_size\n",
    "        self.distilbert.resize_token_embeddings(len(tokenizer))\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.layer_norm = LayerNorm(self.distilbert.config.hidden_size)\n",
    "        self.classifier = nn.Linear(self.distilbert.config.hidden_size, num_labels)\n",
    "        self.class_weights = class_weights  # Store class weights\n",
    "        if self.class_weights is not None:\n",
    "            self.class_weights = self.class_weights.to(self.distilbert.device)\n",
    "        self.loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0, :]  # [CLS] token representation\n",
    "\n",
    "        hidden_state = self.layer_norm(hidden_state)\n",
    "\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "\n",
    "        logits = self.classifier(hidden_state)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a53f4ded-6c36-4ae7-a20d-dacdd8d1ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fffe442-161a-4237-8945-2f16e684372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "summarizer = pipeline(\"summarization\", device=device)\n",
    "\n",
    "def summarize_text(text):\n",
    "    summary = summarizer(text, max_length=5, min_length=5, do_sample=False)\n",
    "    return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d57a7e1-2bef-4766-b002-1e1727b0e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "df['sentence'] = df['title'].fillna('[MISSING_TITLE]') + ' / ' + df['content'].fillna('[MISSING_CONTENT]')\n",
    "df = df.drop(['title', 'content'], axis=1)\n",
    "\n",
    "texts = [(text.split(' / ', 1)[0], text.split(' / ', 1)[1]) for text in df['sentence']]\n",
    "labels = df['class'].to_list()\n",
    "try:\n",
    "    labels = [0 if train == False else 1 for train in labels]\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "try:\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        texts, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ebe1dbf-bcac-4017-917d-8c4396b350df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_test['sentence'] = df_test['title'].fillna('[MISSING_TITLE]') + ' / ' + df_test['content'].fillna('[MISSING_CONTENT]')\n",
    "df_test = df_test.drop(['title', 'content'], axis=1)\n",
    "cols = df_test.columns.to_list()\n",
    "df_test = df_test[cols]\n",
    "\n",
    "test_texts = [(text.split(' / ', 1)[0], text.split(' / ', 1)[1]) for text in df_test['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97707a29-9076-4175-86f0-1d101a786b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e66996b1-385d-465f-8705-d8e6c7702022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Tokenizing Texts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 883/883 [00:18<00:00, 48.08it/s]\n",
      "Batch Tokenizing Texts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 221/221 [00:05<00:00, 43.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def batch_tokenize_texts(texts, tokenizer, batch_size=64):\n",
    "    all_encodings = {}\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Batch Tokenizing Texts\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        try:\n",
    "            batch_encodings = tokenizer(\n",
    "                batch_texts,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=512\n",
    "            )\n",
    "            for key in batch_encodings:\n",
    "                if key not in all_encodings:\n",
    "                    all_encodings[key] = []\n",
    "                all_encodings[key].extend(batch_encodings[key])\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during batch tokenization at batch index {i}: {e}\")\n",
    "    return all_encodings\n",
    "\n",
    "# Tokenize training texts in batches\n",
    "train_encodings = batch_tokenize_texts(train_texts, tokenizer)\n",
    "\n",
    "# Tokenize test texts in batches\n",
    "test_encodings = batch_tokenize_texts(val_texts, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1619caea-4db5-4704-8d00-af881b147e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall transformers accelerate torch -y\n",
    "# !pip install --upgrade 'transformers[torch]'\n",
    "# !pip show accelerate\n",
    "# !pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb34635b-2871-4a45-bb9e-88ea357a463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    # Predictions and true labels\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    \n",
    "    # Calculate balanced accuracy\n",
    "    balanced_acc = balanced_accuracy_score(labels, preds)\n",
    "    \n",
    "    # Calculate other metrics for more comprehensive evaluation\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')       # Weighted F1 for class imbalance\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75a0cd37-6d19-45b2-87c3-996c0fab1cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=128,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    fp16=True,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True, \n",
    "    warmup_ratio=0.4,\n",
    "    learning_rate=2e-5,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa003b25-5976-4f29-8125-a972ff97c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36cfa1a4-9109-4cda-8d49-d513f2e840b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataset: 56460\n",
      "Length of training dataset: 14115\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "test_dataset = CustomDataset(test_encodings, val_labels)\n",
    "print(f\"Length of training dataset: {len(train_dataset)}\")\n",
    "print(f\"Length of training dataset: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b926ad5e-33c6-418d-b35d-33c0569815a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "824737cc-67f8-4688-9ddc-9b6be890c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Assuming 'train_dataset' is your training dataset and it has a 'labels' attribute\n",
    "labels = train_dataset.labels  # Replace with the correct attribute or method to get labels\n",
    "\n",
    "# Compute class weights using sklearn\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "\n",
    "# Convert class weights to tensor and move to device\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5396724a-fdc7-4250-81e5-d72a05d267ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RomanianBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(50002, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RomanianBertForSequenceClassification(num_labels=2, class_weights=class_weights)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca8f2811-1f46-4712-b704-e0a02ee52dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='218' max='6181' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 218/6181 02:37 < 1:12:26, 1.37 it/s, Epoch 0.25/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      7\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping],\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.12/site-packages/transformers/trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.12/site-packages/transformers/trainer.py:2479\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2473\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2474\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2477\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2479\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2480\u001b[0m ):\n\u001b[1;32m   2481\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2482\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac4733d7-f341-4e2b-8780-19e71c2b78a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='111' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [111/111 00:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03775281086564064, 'eval_balanced_accuracy': 0.9924236332570624, 'eval_accuracy': 0.9910024796315976, 'eval_f1': 0.9910217847989077, 'eval_precision': 0.9911503697619318, 'eval_recall': 0.9910024796315976, 'eval_runtime': 53.6457, 'eval_samples_per_second': 263.115, 'eval_steps_per_second': 2.069, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "689aa52b-1da0-420f-94d4-d22e01a00ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b491c200-ab36-44b3-bf32-66d7afa91915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34834/828433816.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RomanianBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(50002, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff98d78d-aec5-4160-8c28-5ba84dc57b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Tokenizing Texts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 573/573 [00:08<00:00, 64.81it/s]\n"
     ]
    }
   ],
   "source": [
    "last_test_encodings = batch_tokenize_texts(test_texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3e70547-f148-4c83-8143-ba96bd1636c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset class for test data\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e172b6f3-8299-4138-9587-bd9c7782f260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36664</th>\n",
       "      <td>36664</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36665</th>\n",
       "      <td>36665</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36666</th>\n",
       "      <td>36666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36667</th>\n",
       "      <td>36667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36668</th>\n",
       "      <td>36668</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36669 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  class\n",
       "0          0      0\n",
       "1          1      1\n",
       "2          2      0\n",
       "3          3      1\n",
       "4          4      0\n",
       "...      ...    ...\n",
       "36664  36664      0\n",
       "36665  36665      0\n",
       "36666  36666      0\n",
       "36667  36667      0\n",
       "36668  36668      0\n",
       "\n",
       "[36669 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "test_dataset = TestDataset(last_test_encodings)\n",
    "\n",
    "# Create a DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=data_collator)\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "df_test['class'] = predictions\n",
    "df_test = df_test.drop(columns=['sentence'])\n",
    "\n",
    "# Save to CSV\n",
    "df_test.to_csv('test_predictions2.csv', index=False)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d11a8e-7208-4d1e-9a6e-52afde3b9704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4900fb7a-fc8a-4b47-8ace-9ea44b36f587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde97cb-054f-4332-95b7-3fc868a81cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
